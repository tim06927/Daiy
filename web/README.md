# Daiy Web App

A Flask-based web interface for AI-powered bike component recommendations with a modular frontend and three-phase LLM workflow.

## Architecture Overview

### Data Flow: Scraper → Database → Web App

```
┌──────────┐      writes       ┌─────────────────┐      queries      ┌─────────────┐
│  Scraper │  ─────────────>   │  products.db    │  <──────────────  │   Web App   │
│ (scrape/)│                    │   (SQLite)      │                   │   (web/)    │
└──────────┘                    │                 │                   └─────────────┘
                                │ • 45.6 MB file  │
                                │ • 11k+ products │
                                │ • Indexed       │
                                │ • On-demand     │
                                └─────────────────┘
```

**Key Benefits:**
- **Memory Efficient**: Queries products on-demand instead of loading all into RAM
- **Scalable**: Can handle 100k+ products without memory issues
- **Shared Database**: Scraper and web app use the same SQLite database
- **Pre-Built Database**: Requires a populated `products.db` (e.g., generated by the scraper); no automatic CSV import on first run

### Three-Phase LLM Flow

```
User Input (text + optional image)
           ↓
[Phase 1: Job Identification]
  - LLM generates step-by-step instructions with [category_key] references
  - LLM self-assesses confidence for each technical specification
  - Specs with confidence < 0.8 flagged as "unclear_specifications"
           ↓
  [Decision: Any unclear specs?]
    ├─ NO: Skip to Phase 3
    └─ YES: Show all questions at once (Phase 2)
           ↓
[Phase 2: Clarification (if needed)]
  - Display all unclear questions with:
    • Question text (LLM-generated)
    • Hint (how to find the answer)
    • 2-5 options (multiple choice)
    • "Other" button (custom text)
  - User can answer in any order
  - All answers collected at once
           ↓
[Phase 3: Final Recommendation]
  - LLM receives instructions, answers, and available products
  - Replaces [category_key] with actual product names
  - Returns:
    • final_instructions: Step-by-step with product names
    • primary_products: What user explicitly needs
    • tools: Required for the job
    • optional_extras: Max 3 related items
           ↓
Display Results with Answered Questions
  - Final instructions rendered as numbered steps
  - Products organized by type with per-item reasoning
  - Left panel shows all answered questions as labeled bubbles
```

### Key Capabilities

- **Dynamic Clarification** - LLM identifies which specifications are unclear (confidence < 0.8) and generates targeted questions
- **Flexible Questions** - Any number of specifications can be asked, not limited to predefined dimensions
- **Comprehensive Logging** - All interactions logged with event types:
  - `user_input` - Initial query + clarification answers
  - `clarification_required` - Questions being asked
  - `llm_call_*` / `llm_response_*` - All LLM interactions
  - `recommendation_result` - Final output with summary
- **HTML Log Viewer** - `view_logs.py` displays logs by session with filters
- **Modular Frontend** - Separate CSS and JavaScript files for maintainability (see `static/README.md`)

### Current Limitations & Future Improvements

**Current State (Production-Ready):**
- ✅ Three-phase LLM flow fully implemented and tested
- ✅ Dynamic clarification questions generated by LLM
- ✅ Multi-category product support (products can belong to multiple categories)
- ✅ Comprehensive logging with HTML viewer
- ✅ Modular frontend architecture
- ✅ Image processing with vision capabilities
- ✅ Grounded product recommendations (no hallucination)

**Known Limitations:**
- Optional products only suggested from already-mentioned categories
- Product images may not be available for all items (fallback icons used)
- Single-turn conversation only (no multi-turn dialogue)
- UI uses full-page refresh between phases (frontend state reset)

## Features

- **Natural language input** - Describe your project in plain text
- **Image upload** - Optional bike photo for visual analysis
- **Smart clarification** - AI infers missing info or asks targeted questions with helpful hints
- **Split-panel UI** - Query context on left, results on right
- **Per-product explanations** - Each product shows "why it fits" your needs
- **Tabbed results** - Products and installation instructions in separate tabs
- **Grounded recommendations** - Only suggests real products in inventory
- **Direct links** - One-click access to bike-components.de product pages
- **Optional basic auth** - Protect demos with username/password

## Project Structure

```
web/
├── app.py              # Flask app setup and routes
├── api.py              # API endpoints (/api/recommend, /api/categories)
├── categories.py       # Product category definitions (queries database)
├── candidate_selection.py  # Product filtering logic (database queries)
├── catalog.py          # Database query functions (SQLite backend)
├── config.py           # Centralized configuration
├── image_utils.py      # Image processing for OpenAI (with resizing)
├── job_identification.py  # LLM-based job/category identification
├── logging_utils.py    # Interaction logging
├── prompts.py          # Prompt building for LLM calls
├── static/             # Frontend assets (modular structure)
│   ├── css/            # Stylesheets
│   │   ├── base.css            # Variables, resets, layout
│   │   ├── components.css      # Forms, buttons, panels
│   │   └── products.css        # Product cards, categories
│   ├── js/             # JavaScript modules
│   │   ├── config.js           # Constants
│   │   ├── state.js            # State management
│   │   ├── image.js            # Image handling
│   │   ├── api.js              # Backend communication
│   │   ├── clarification.js    # Clarification UI
│   │   ├── products.js         # Product rendering
│   │   └── main.js             # Initialization
│   └── README.md       # Frontend architecture guide
├── templates/
│   └── index.html      # Clean HTML template (154 lines)
├── logs/               # LLM interaction logs (JSONL)
├── tests/              # Test files
│   ├── test_model_clarification.py
│   ├── test_model_clarification_extended.py
│   └── test_vision_flow.py
└── README.md           # This file
```

## Files

### Core Modules (Three-Phase Flow)

#### `job_identification.py` - Phase 1: Understand the Job
Analyzes user input to extract what needs to be done:
- `UnclearSpecification` - Represents a spec needing clarification
  - Fields: spec_name, confidence, question, hint, options
  - Used to flag unclear specs (confidence < 0.8)
- `JobIdentification` - Result of job identification
  - `instructions` - List of steps with `[category_key]` placeholders
  - `unclear_specifications` - Array of unclear specs
  - `inferred_values` - Detected specifications (e.g., {"use_case": "road"})
  - `confidence` - Overall confidence (0-1)
  - `reasoning` - Why these categories/specs were detected
- `identify_job()` - Calls LLM with problem text and optional image
- `extract_categories_from_instructions()` - Parses `[category_key]` references

**Key change:** Instructions now contain actual work steps, not category lists.

#### `api.py` - Orchestrate Three Phases
Main API endpoint `/api/recommend` orchestrates all phases:
1. **Phase 1 call** - `identify_job()` to analyze problem
2. **Check clarifications** - Filter unanswered unclear specs
3. **Return if needed** - Send clarification_questions back to frontend
4. **Phase 2 done** - User answers and sends back with `clarification_answers`
5. **Phase 3 call** - `_call_llm_recommendation()` with answers and products
6. **Return results** - final_instructions with primary_products, tools, optional_extras

Also handles:
- Image processing for OpenAI
- Product catalog validation
- Candidate selection for recommendation phase

#### `prompts.py` - LLM Prompt Builders
- `_build_job_identification_prompt()` - Phase 1
  - Asks LLM to generate step-by-step instructions
  - Mentions categories for context (uses `[category_key]` format)
  - Asks LLM to self-assess confidence for each spec
- `build_recommendation_context()` - Assemble Phase 3 context
  - Combines instructions, clarifications, available products
  - Formatted as JSON for LLM parsing
- `_make_recommendation_prompt_new()` - Phase 3 prompt
  - Asks LLM to finalize instructions (replace placeholders)
  - Select primary products, tools, optional extras (max 3)
  - Provide reasoning for each selection

### Supporting Modules

#### `categories.py`
Product category system with **dynamic discovery**:
- `PRODUCT_CATEGORIES` - Dict mapping category_key → config (auto-generated from CSV)
  - display_name, description, fit_dimensions, product_count
- `discover_categories_from_catalog()` - Reads CSV at startup, extracts all unique categories
- `CATEGORY_OVERRIDES` - Special handling for drivetrain (gearing-based filtering)
- `CATEGORY_DIMENSION_PATTERNS` - Pattern-based inference of fit dimensions
- `SHARED_FIT_DIMENSIONS` - Common specs across categories
  - gearing, use_case, brake_rotor_diameter, freehub_compatibility, etc.
  - Each has: prompt, hint, options
- `refresh_categories()` - Reload categories at runtime after scraping

**Note:** Categories are discovered automatically from the product database (~205 categories). No manual category registration needed.

#### `candidate_selection.py`
Product filtering for Phase 3:
- `select_candidates_dynamic()` - Filters products by category + fit values
- Respects `MAX_CHAINS/CASSETTES/TOOLS` limits (from config)
- Returns products ready for LLM recommendation

#### `config.py`
Centralized configuration:
- `LLM_MODEL` - OpenAI model (gpt-4-mini)
- `FLASK_HOST/PORT` - Server settings
- Product data paths and candidate limits

#### `logging_utils.py`
Structured JSONL logging:
- `log_interaction()` - Write timestamped event to log
- LOG_FILE - Path to today's log file
- Event types: user_input, clarification_required, llm_call_*, llm_response_*, recommendation_result, llm_parse_error, llm_error, etc.

#### `view_logs.py`
HTML log viewer:
- Reads JSONL log file
- Groups events by session (user interaction sessions)
- Creates formatted HTML report
- Supports filtering by event type
- Collapsible details for prompts/responses
- Mobile-friendly styling

Usage: `python web/view_logs.py` opens view_logs.html in browser

#### `image_utils.py`
Image processing for OpenAI:
- Validates base64 image format
- Resizes images to 2048x2048 max (reduces memory usage)
- Converts to PNG format
- Provides image metadata for logging

#### `catalog.py`
Database query functions:
- `query_products()` - Query products from SQLite database
- `get_categories()` - Get distinct categories from database
- `get_product_count()` - Count products by category
- On-demand queries (no full data loading)
- Memory efficient: 0.2-5MB per query vs 500MB+ for full CSV

#### `app.py`
Flask application setup:
- Basic auth (optional demo protection)
- Blueprint registration
- GET `/` serves index.html
- Error handling

#### `templates/index.html`
Clean HTML template (154 lines) that references external CSS and JavaScript:
- Structured semantic HTML
- Links to modular CSS files in `static/css/`
- Links to modular JavaScript files in `static/js/`
- Minimal inline styling
- See `static/README.md` for frontend architecture details

#### `static/` - Frontend Assets
Modular frontend architecture with separation of concerns:
- **CSS Modules** (3 files):
  - `base.css` - Variables, resets, layout, header
  - `components.css` - Forms, buttons, clarification panels
  - `products.css` - Product cards, categories, alternatives
- **JavaScript Modules** (7 files):
  - `config.js` - Application constants
  - `state.js` - Centralized state management
  - `image.js` - Image upload and compression
  - `api.js` - Backend API communication
  - `clarification.js` - Clarification UI rendering
  - `products.js` - Product display logic
  - `main.js` - Application initialization

See detailed documentation in `static/README.md`

### App Flow

```
[User Input]
    ↓
identify_job() ──→ LLM Phase 1
    ↓
[Check unclear specs]
    ├─ Yes ──→ Return clarification_questions
    │           ↓
    │         [User selects options]
    │           ↓
    │         [POST /api/recommend again]
    │
    └─ No
        ↓
select_candidates_dynamic() ──→ Get products
        ↓
_call_llm_recommendation() ──→ LLM Phase 3
        ↓
[Return final results]
```

## Setup

### Prerequisites
- Python 3.8+
- OpenAI API key with access to `gpt-5-mini`
- SQLite database at `data/products.db` (auto-created from CSV on first run)

### Installation

```bash
# From project root
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY

# Run the app
python web/app.py
```

The app will start at `http://127.0.0.1:5000`

## Usage

### Web Interface

1. Open http://127.0.0.1:5000
2. Enter your project description (e.g., "I need a 12-speed chain for my road bike")
3. Optionally upload a photo of your bike
4. Click submit →
5. **If clarification needed:**
   - Review the preview instructions
   - Answer all clarification questions (hints provided)
   - Click "Get Recommendations"
6. **View results:**
   - **Products Tab** - Primary products with reasoning, tools, optional extras
   - **Instructions Tab** - Step-by-step instructions with product names
   - **Answered Questions** - Bubbles showing what you clarified

### API Endpoints

#### POST `/api/recommend` - Main Recommendation Flow

**Phase 1 - Initial Request:**
```json
{
  "problem_text": "I need a new 12-speed chain for my road bike",
  "image_base64": null
}
```

**Phase 2 - Response with Clarification Questions:**
```json
{
  "need_clarification": true,
  "job": {
    "instructions": [
      "Step 1: Remove old chain using chain tool [drivetrain_tools]",
      "Step 2: Install new Shimano [drivetrain_chains] chain"
    ],
    "unclear_specifications": [
      {
        "spec_name": "use_case",
        "confidence": 0.4,
        "question": "What type of riding do you do?",
        "hint": "Road, mountain, commuting, touring?",
        "options": ["road", "mountain", "commuting", "touring"]
      }
    ]
  },
  "clarification_questions": [/* same as unclear_specifications */],
  "instructions_preview": ["Step 1: Remove old chain...", "Step 2: Install new chain..."],
  "inferred_values": {"gearing": 12}
}
```

**User Answers & Follow-up Request:**
```json
{
  "problem_text": "I need a new 12-speed chain for my road bike",
  "clarification_answers": [
    {"spec_name": "use_case", "answer": "road"}
  ],
  "identified_job": {...}  // From previous response
}
```

**Phase 3 - Final Response with Products:**
```json
{
  "diagnosis": "12-speed chain replacement for road bike",
  "final_instructions": [
    "Step 1: Use Park Tool CT-3.2 chain tool to remove old Shimano CN-M7000 chain",
    "Step 2: Install new Shimano CN-M8100 12-speed chain",
    "Step 3: Connect with quick-link, ensuring proper direction"
  ],
  "primary_products": [
    {
      "category": "drivetrain_chains",
      "category_display": "Chains",
      "product": {
        "name": "Shimano CN-M8100",
        "brand": "Shimano",
        "price": "$29.99",
        "url": "https://bike-components.de/..."
      },
      "reasoning": "12-speed chain compatible with Shimano drivetrain and road use"
    }
  ],
  "tools": [
    {
      "category": "drivetrain_tools",
      "category_display": "Tools",
      "product": {
        "name": "Park Tool CT-3.2",
        "brand": "Park Tool",
        "price": "$24.99",
        "url": "https://bike-components.de/..."
      },
      "reasoning": "Required to safely remove and install chain"
    }
  ],
  "optional_extras": [
    {
      "category": "drivetrain_cassettes",
      "category_display": "Cassettes",
      "product": {...},
      "reasoning": "Worn chains often damage cassettes - consider replacing together"
    }
  ],
  "fit_values": {"gearing": 12, "use_case": "road"}
}
```

**Key Differences from Phase 2:**
- `final_instructions` replaces `[category_key]` with actual product names
- `primary_products`, `tools`, `optional_extras` - organized by purpose
- Each product includes per-item `reasoning`
- Max 3 `optional_extras` from already-mentioned categories

#### GET `/api/categories` - List Categories

**Response:**
```json
{
  "categories": [
    {
      "key": "drivetrain_chains",
      "display_name": "Chains",
      "description": "Bicycle chains for different speeds",
      "fit_dimensions": ["gearing", "use_case"]
    },
    {
      "key": "drivetrain_tools",
      "display_name": "Drivetrain Tools",
      "description": "Tools for chain, cassette, and drivetrain work",
      "fit_dimensions": []
    }
  ]
}
```

---

## Configuration

Environment variables (see `.env.example` in project root):

```bash
OPENAI_API_KEY=sk-...        # Required
FLASK_HOST=0.0.0.0           # Optional (default: 0.0.0.0)
FLASK_PORT=5000              # Optional (default: 5000)
FLASK_DEBUG=True             # Optional (default: False)
DEMO_USER=demo               # Optional basic auth
DEMO_PASS=changeme           # Optional basic auth
MAX_CASSETTES=5              # Optional (default: 5)
MAX_CHAINS=5                 # Optional (default: 5)
MAX_TOOLS=5                  # Optional (default: 5)
```

## How It Works

### Data Flow

1. **Load Catalog** - App loads CSV on startup, derives speed from product names
2. **User Input** - User describes their project (text + optional image)
3. **Regex Inference** - Extract speed/use-case from text patterns
4. **LLM Inference** - If regex fails, ask LLM to infer or propose options
5. **Clarification** - If still unclear, show options with helpful hints
6. **Select Candidates** - Filter products by speed and use case
7. **Build Context** - Create JSON with filtered products for LLM
8. **Generate Recommendation** - LLM picks best products with explanations
9. **Display Results** - Show product cards + installation instructions

### Grounding Pattern

The prompt includes:
- User's problem description
- Detected bike specs (speed, use case)
- **Candidate products only** (real inventory)
- Explicit instruction: "recommend from the provided candidates only"
- Structured JSON output format

This ensures:
- No hallucinated products
- All URLs are real and verifiable
- Recommendations respect technical constraints
- Per-product explanations for transparency

## Logging

All LLM interactions are logged to `web/logs/llm_interactions_YYYYMMDD.jsonl`:
- User inputs and image metadata
- Regex inference results
- LLM prompts and responses
- Clarification requests

Use `python web/view_logs.py` to inspect logs.

## Troubleshooting

### "No products found"
- Verify `data/products.db` exists
- Run scraper: `python -m scrape.cli`
- Check database has products: `sqlite3 data/products.db "SELECT COUNT(*) FROM products"`

### "OpenAI API error"
- Check `OPENAI_API_KEY` in `.env` file
- Verify API key has access to gpt-5-mini
- Check API quota and usage limits

### "need_clarification keeps appearing"
- Be more specific: "12-speed gravel bike" instead of "my bike"
- Or select from the provided options

## Performance Tips

- **Catalog caching** - Loaded once at startup
- **Candidate limits** - MAX_CASSETTES/CHAINS/TOOLS reduce context size
- **Early filtering** - `select_candidates()` filters before LLM call
- **Single LLM call** - Recommendation uses one API call

## Performance Tracking

The app includes built-in performance timing to measure LLM vs app latency:

**Track timing in code:**
```python
from web.timing import timer, get_timings, reset_timings

with timer("llm_call"):
    response = openai_client.chat.completions.create(...)

timings = get_timings()
print(f"LLM: {timings['__summary__']['llm_percent']}%")
```

**Analyze logs:**
```bash
python web/view_performance.py --days 7
```

Output shows:
- Mean/P50/P95/P99 latencies for LLM and app operations
- Percentage breakdown of total request time
- Per-operation timing details (job_identification, clarification, recommendation, candidate_selection, etc.)

**Integration:**
- All `/api/recommend` requests automatically tracked
- Metrics logged to JSONL with `request_id` for correlation with other events
- Request ID enables linking timing data to user_input, clarification_required, and recommendation_result events

**Performance baselines:**
- Job Identification (LLM): 0.3-1.0s
- Recommendation (LLM): 1.0-5.0s
- Candidate Selection (App): 20-500ms
- Total request: 1.5-6.0s

For implementation details, see `timing.py` and `view_performance.py` docstrings.

## Memory Optimization

**Database-backed queries** (replacing CSV loading):
- Queries products on-demand from SQLite instead of loading all into RAM
- Reduces startup memory from 800MB+ to <200MB
- Enables 512MB Render deployment tier with 100k+ products
- Connection pooling and indexed queries for performance

**Key modules:**
- `catalog.py` - Database query functions (query_products, get_categories)
- `candidate_selection.py` - Efficient filtering with product limits
- `categories.py` - Dynamic discovery from database with caching

**Memory-efficient patterns:**
- No full catalog loading - only query what's needed
- Product iteration avoids list concatenation
- Temporary result filtering with generators where possible
- One database connection per request with automatic cleanup

Results: ~150-200MB steady-state memory (vs 800MB+ with CSV loading), handles 100k+ products reliably.

## Error Tracking & Monitoring

Comprehensive error logging system for alpha deployments (especially on Render):

### Overview

**Problem:** On Render, ephemeral filesystems don't persist log files across redeployments, making it impossible to access error logs after the deployment updates.

**Solution:** Store all errors in persistent SQLite database (`data/products.db`) alongside product data. Errors survive redeployments and are queryable via CLI tool.

### Error Types

All errors logged with full context, stack traces, and recovery suggestions:

- **llm_error** - OpenAI API errors, JSON parsing failures, model errors
  - Common causes: API quota exceeded, invalid response format, network issues
  - Includes: HTTP status, error message, recovery suggestion

- **validation_error** - Invalid user input, constraint violations, type mismatches
  - Common causes: Missing required fields, invalid problem_text, malformed JSON
  - Includes: Field name, expected vs actual value, recovery suggestion

- **database_error** - Query failures, connection issues, missing data
  - Common causes: Database locked, query timeout, constraint violation
  - Includes: Query details, operation context, recovery suggestion

- **processing_error** - Image processing failures, file operations
  - Common causes: Invalid image format, file too large, codec issues
  - Includes: File details, error stage, recovery suggestion

- **unexpected_error** - Uncaught exceptions with full stack trace
  - Common causes: Bugs, edge cases, unhandled scenarios
  - Includes: Full stack trace, operation context, request_id for debugging

### Database Schema

Errors stored in `error_log` table (created automatically):

```sql
CREATE TABLE error_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp TEXT NOT NULL,           -- ISO 8601 timestamp
    request_id TEXT NOT NULL,          -- UUID correlating to other events
    error_type TEXT NOT NULL,          -- llm_error, validation_error, etc.
    error_message TEXT NOT NULL,       -- Error summary
    stack_trace TEXT,                  -- Full traceback for unexpected errors
    context JSON,                      -- Operation context (as JSON)
    operation TEXT,                    -- What was being done (e.g., "validate_input")
    phase TEXT,                        -- LLM phase (1, 2, or 3)
    user_input TEXT,                   -- Original problem text (truncated)
    timing_data JSON,                  -- Performance metrics if available
    recovery_suggestion TEXT           -- How to fix or retry
);

CREATE INDEX idx_timestamp ON error_log(timestamp);
CREATE INDEX idx_request_id ON error_log(request_id);
CREATE INDEX idx_error_type ON error_log(error_type);
```

### Access Error Logs

#### View All Errors

```bash
python web/view_errors.py
```

Shows summary statistics:
- Total errors, count by type, 24-hour count
- Top error messages
- Recent errors with details

#### Filter by Request ID

Trace entire transaction for a specific user request:
```bash
python web/view_errors.py --request abc123def456
```

Shows all errors for that request_id, with:
- Timestamp, error type, message
- Operation context (which phase, what parameters)
- Stack trace (last 10 lines)
- Recovery suggestion

#### Filter by Error Type

```bash
python web/view_errors.py --type llm_error          # All LLM errors
python web/view_errors.py --type validation_error   # Input validation errors
python web/view_errors.py --type database_error     # Database query errors
```

#### List Recent Errors with Pagination

```bash
python web/view_errors.py --all                     # List all (paginated)
python web/view_errors.py --all --limit 50          # Show 50 errors
python web/view_errors.py --all --offset 50         # Skip first 50
```

#### Export for Analysis

Export all errors to JSON/JSONL:

```bash
# Export as JSON (pretty-printed)
python web/view_errors.py --export json --output errors.json

# Export as JSONL (one error per line, better for stream processing)
python web/view_errors.py --export jsonl --output errors.jsonl
```

Exported format:
```json
{
  "id": 123,
  "timestamp": "2025-01-21T14:30:45.123Z",
  "request_id": "550e8400-e29b",
  "error_type": "llm_error",
  "error_message": "OpenAI API rate limit exceeded",
  "operation": "llm_recommendation_phase_3",
  "phase": "3",
  "context": {"model": "gpt-4-mini", "status_code": 429},
  "recovery_suggestion": "Wait 60 seconds and retry"
}
```

### Integration with Logging

Errors are correlated with existing event logging via `request_id`:

**Flow of a single user request:**

```
user_input event          ←─────┐
   │                            │
   ├─→ clarification_required    ├─ Same request_id
   │       (optional)            │
   │                            │
   ├─→ llm_call_phase_1         │
   ├─→ llm_response_phase_1     │
   │                            │
   ├─→ llm_call_phase_3         │
   ├─→ llm_response_phase_3     │
   │                            │
   ├─→ performance_metrics      │
   │                            │
   └─→ recommendation_result ←──┘

If any error occurs during the request:
   ├─→ error event (llm_error, validation_error, etc.)
       - Same request_id for tracing
       - Includes timing_data up to error point
```

All events logged to both:
1. **JSONL files** - `web/logs/llm_interactions_YYYYMMDD.jsonl` (human-readable, local development)
2. **SQLite database** - `data/products.db` (persistent across redeployments, for errors only)

Use `request_id` to correlate errors with associated user_input and LLM calls.

### Implementation Details

#### error_logging.py

Main error logging module:

```python
from web.error_logging import (
    log_llm_error,
    log_validation_error,
    log_database_error,
    log_processing_error,
    log_unexpected_error,
)

# Log an LLM error with recovery suggestion
log_llm_error(
    "OpenAI API rate limited",
    request_id="550e8400-e29b",
    operation="llm_recommendation",
    context={"status_code": 429},
    recovery_suggestion="Wait 60 seconds and retry"
)

# Or catch and log an exception
try:
    response = openai_client.chat.completions.create(...)
except Exception as e:
    log_llm_error(
        str(e),
        request_id=request_id,
        operation="llm_recommendation",
        context={"model": "gpt-4-mini"},
    )
```

#### view_errors.py

CLI tool for querying and exporting errors:

```bash
# Show summary and recent errors
python web/view_errors.py

# Filter by error type
python web/view_errors.py --type llm_error

# Trace specific request
python web/view_errors.py --request 550e8400-e29b

# Export to JSON for analysis
python web/view_errors.py --export json --output errors.json

# Get help
python web/view_errors.py --help
```

#### api.py Integration

All API errors logged automatically:

- **JSON parsing** → `validation_error` with recovery suggestion
- **Input validation** → `validation_error` with field details
- **Image processing** → `processing_error` with file details
- **Database queries** → `database_error` with operation context
- **LLM calls** → `llm_error` with API status
- **Unexpected** → `unexpected_error` with full stack trace

Example from recommend() endpoint:

```python
try:
    # Phase 1: Identify job
    job = identify_job(problem_text, request_id=request_id)
except Exception as e:
    log_llm_error(
        str(e),
        request_id=request_id,
        operation="job_identification",
        phase="1",
        context={"error_type": type(e).__name__},
        recovery_suggestion="Check image format and problem text",
    )
    return jsonify({"error": "...", "request_id": request_id}), 400
```

### Monitoring Workflow

**For development (local):**
1. Run app: `python web/app.py`
2. View interactions: `python web/view_logs.py`
3. View errors: `python web/view_errors.py`

**For Render deployment:**
1. Connect to Render (SSH or logs)
2. Query errors: `python web/view_errors.py --all`
3. Filter by request: `python web/view_errors.py --request <id>`
4. Export for analysis: `python web/view_errors.py --export json`
5. Download exported file for offline analysis

**Alert patterns to watch:**
- Spike in `llm_error` → Check API quota, rate limits
- `validation_error` → Review user inputs, edge cases
- Specific `error_message` repeated → Likely a bug or data issue
- `processing_error` with image errors → Image format/size issues

### Render Deployment Persistence

**Why SQLite solves the persistence problem:**

| Storage Method | Persists Across Redeploy? | Query Support | External Service? |
|---|---|---|---|
| Log files (JSONL) | ❌ Ephemeral filesystem | No (manual search) | No |
| Environment DB (ephemeral) | ❌ Clears on redeploy | Yes | No |
| **SQLite on disk** | ✅ Stored with app data | ✅ SQL queries | No |
| Cloud service (Datadog, etc.) | ✅ Yes | ✅ Yes | ✅ Costs $$ |
| Cloud storage (S3) | ✅ Yes | Manual | ✅ Costs $$ |

**Implementation:** Errors written to `error_log` table in `data/products.db` (same database as products). Since Render preserves `/data` directory between deployments, all errors persist indefinitely.

### Testing Error Logging

Generate test errors to verify system:

```bash
# In Python shell
from web.error_logging import log_llm_error
import uuid

request_id = str(uuid.uuid4())

log_llm_error(
    "Test LLM error for validation",
    request_id=request_id,
    operation="test",
    context={"test": True},
    recovery_suggestion="This is a test error"
)

# View the error
# python web/view_errors.py --request <request_id>
```

Or trigger real errors by:
1. Sending invalid JSON to `/api/recommend`
2. Submitting empty problem_text
3. Uploading invalid image format
4. Querying with impossible product specifications

All errors will be logged and visible via `python web/view_errors.py`.

## Interaction Logging

Complete audit trail of all LLM interactions, from initial user input through final recommendations. All events stored in persistent SQLite database (`data/products.db`) for production visibility and local development debugging.

### Event Types

Every interaction is tagged with an `event_type` and `request_id` for request tracing:

| Event Type | When Fired | Data Logged | Use Case |
|---|---|---|---|
| `user_input` | Request begins | problem_text, image size, clarification_answers | Audit trail of user requests |
| `clarification_required` | Phase 1 or 2 | unclear_specs, confidence_score, questions asked | Understand why clarifications were needed |
| `llm_call_phase_1` | Before identifying job | model, prompt details, parameters | Debug LLM behavior |
| `llm_response_phase_1` | After identifying job | identified_job, use_case, gearing, cost_range | Verify job identification accuracy |
| `llm_call_phase_3` | Before generating recommendations | model, final_instructions, product_count | Verify product matching |
| `llm_response_phase_3` | After recommendations generated | primary_products, recommended_tools, sections | Track final output |
| `recommendation_result` | Request completes | product_rankings, sections, generation time | Complete transaction audit |
| `performance_metrics` | After completion (optional) | total_time, phase_times, api_calls, tokens_used | Performance analysis |

### Database Schema

Interactions stored in `interactions` table (created automatically):

```sql
CREATE TABLE interactions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp TEXT NOT NULL,           -- ISO 8601 timestamp
    request_id TEXT NOT NULL,          -- UUID linking all events for one request
    event_type TEXT NOT NULL,          -- user_input, llm_call_phase_1, etc.
    data JSON NOT NULL                 -- Event-specific data as JSON
);

CREATE INDEX idx_interactions_request ON interactions(request_id);
CREATE INDEX idx_interactions_type ON interactions(event_type);
CREATE INDEX idx_interactions_timestamp ON interactions(timestamp);
```

### Request Tracing

All events for a single user request share the same `request_id`. This enables complete request tracing:

**Example trace for single recommendation request:**

```
Request ID: 550e8400-e29b-41d4-a716-446655440000

[2025-01-22 10:15:30.123] user_input
  - problem_text: "Chain keeps slipping on cassette 8 and 9"
  - has_image: true
  - image_size: 245382 bytes

[2025-01-22 10:15:32.456] llm_call_phase_1
  - model: "gpt-4-mini"
  - prompt_tokens: 1243

[2025-01-22 10:15:35.789] llm_response_phase_1
  - identified_job: "improve_shifting"
  - use_case: "urban_commuting"
  - gearing: "3x9"
  - cost_range: "budget"

[2025-01-22 10:15:36.123] clarification_required
  - unclear_specs: ["drivetrain_condition", "shifting_speed_preference"]
  - confidence: 0.72

[2025-01-22 10:15:38.789] llm_call_phase_3
  - model: "gpt-4-mini"
  - product_candidates: 45

[2025-01-22 10:15:42.012] llm_response_phase_3
  - primary_products: 3
  - recommended_tools: 2
  - sections: 4

[2025-01-22 10:15:42.345] recommendation_result
  - total_time_ms: 12222
  - status: "success"

[2025-01-22 10:15:42.678] performance_metrics
  - phase_1_time_ms: 5233
  - phase_3_time_ms: 3876
  - total_api_calls: 2
  - total_tokens: 3456
```

### Access Interaction Logs

#### View All Interactions

```bash
python web/view_logs.py --db sqlite
```

Shows HTML viewer with all events from past 24 hours, event type, timestamp, request_id, and summary statistics.

#### Filter by Request ID

Retrieve complete request trace:

```bash
python web/view_logs.py --db sqlite --request 550e8400-e29b-41d4-a716-446655440000
```

Returns all events for that request_id in chronological order. Essential for debugging specific user issues.

#### Filter by Event Type

Find all events of a specific type:

```bash
python web/view_logs.py --db sqlite --type user_input
python web/view_logs.py --db sqlite --type clarification_required
python web/view_logs.py --db sqlite --type llm_call_phase_1
python web/view_logs.py --db sqlite --type recommendation_result
```

Useful for analyzing patterns (e.g., "How many requests needed clarification?").

#### Using Makefile Shortcuts

```bash
make interactions                          # View all interactions
make interactions-request ID=550e8400-e29b # View request trace
make interactions-type TYPE=user_input     # Filter by type
```

### Querying Interactions Programmatically

Direct Python access via `error_logging` module:

```python
from web.error_logging import ErrorLogger

logger = ErrorLogger()

# Get all interactions for a request
trace = logger.get_interaction_trace(request_id="550e8400-e29b")
for event in trace:
    print(f"{event['timestamp']} {event['event_type']}")

# Get summary statistics
summary = logger.get_interaction_summary()
print(f"Total: {summary['total']}")
print(f"By type: {summary['by_type']}")
```

### Integration with Error Tracking

Errors and interactions stored in same database, linked by `request_id`:

```bash
# Get complete trace including errors
python web/view_logs.py --db sqlite --request 550e8400-e29b
python web/view_errors.py --request 550e8400-e29b
```

Correlate errors with user input to debug specific issues:

```python
from web.error_logging import ErrorLogger

logger = ErrorLogger()
trace = logger.get_interaction_trace(request_id)

# Find user input and any errors
user_input = next(e for e in trace if e['event_type'] == 'user_input')
errors = logger.get_errors(request_id=request_id)

print(f"User asked: {user_input['data']['problem_text']}")
for error in errors:
    print(f"Error at {error['operation']}: {error['error_message']}")
```

### Unified Database Architecture

All application state stored in single `data/products.db` SQLite database:

| Table | Purpose |
|---|---|
| `products` | Bike components from scraper |
| `error_log` | All errors with context |
| `interactions` | All LLM interactions |

**Benefits:**
- ✅ Single backup/restore point
- ✅ Request tracing across errors and interactions
- ✅ No external services required
- ✅ Persists on Render across redeployments
- ✅ Efficient SQL queries
- ✅ Local file storage (no cloud costs)

### Monitoring Workflow

**Local Development:**

```bash
# Terminal 1: Run app
python web/app.py

# Terminal 2: Monitor interactions
python web/view_logs.py --db sqlite

# Terminal 3: Check for errors
python web/view_errors.py --all
```

**Render Deployment Monitoring:**

```bash
# View recent interactions
python web/view_logs.py --db sqlite --limit 100

# Trace specific request
python web/view_logs.py --db sqlite --request <request_id>

# Check for errors
python web/view_errors.py --type llm_error --limit 50
```

### Testing Interaction Logging

Generate test interactions:

```bash
python -c "
import uuid
from web.error_logging import ErrorLogger

logger = ErrorLogger()
request_id = str(uuid.uuid4())

# Log test events
logger.log_interaction('user_input', request_id, {
    'problem_text': 'Test request',
    'has_image': False
})

logger.log_interaction('recommendation_result', request_id, {
    'status': 'success',
    'total_time_ms': 2500
})

# Retrieve and display
trace = logger.get_interaction_trace(request_id)
print(f'✅ Logged {len(trace)} test interactions')
print(f'Request ID: {request_id}')
"
```

View in log viewer:

```bash
python web/view_logs.py --db sqlite --request <request_id>
```

